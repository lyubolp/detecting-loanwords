{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import src.constants as const\n",
    "from src.dataset_utils import load_files, build_vocab_transformation, tokenize_source, tokenize_target\n",
    "from src.transcription_dataset_single_word import TranscriptionDataset\n",
    "from src.transformer_model import Seq2SeqTransformer\n",
    "from src.syllable_splitter import split_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = 5187  # Hardcoded for now\n",
    "TGT_VOCAB_SIZE = 3387  # Hardcoded for now\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(const.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('models/transformer-single-word-2023-11-10-606102-25.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels_transcription = ['a', 'ʌ', 'ɤ̞',  'ɐ', 'ɔ', 'o', 'u', 'ɛ', 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filepath = '/mnt/d/Projects/masters-thesis/data/single_words.txt'\n",
    "\n",
    "with open(words_filepath, 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "amount_of_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & test split\n",
    "sentences_to_use = amount_of_words\n",
    "train_split = int(const.TRAIN_TEST_SPLIT * sentences_to_use)\n",
    "validation_split = int((const.TRAIN_TEST_SPLIT + const.TRAIN_VALIDATION_SPLIT) * sentences_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranscriptionDataset(words_filepath, tokenization_src=split_word, \n",
    "                                     tokenization_tgt=lambda x: split_word(x, vowels_transcription),\n",
    "                                     start_index=0, end_index=train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: list[int]):\n",
    "    return torch.cat((torch.tensor([const.BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([const.EOS_IDX])))\n",
    "\n",
    "const.vocab_transform[const.SRC_LANGUAGE] = build_vocab_transformation(train_dataset, const.SRC_LANGUAGE)\n",
    "\n",
    "text_transform_src = sequential_transforms(tokenize_source,\n",
    "    const.vocab_transform[const.SRC_LANGUAGE], #Numericalization\n",
    "                                                tensor_transform) # Add BOS/EOS and create tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model: Seq2SeqTransformer, word: str):\n",
    "    word = word.lower()\n",
    "    model.eval()\n",
    "    src = text_transform_src(word).view(-1, 1).to(const.device)\n",
    "\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    \n",
    "    embedding = model.encode(src.to(const.device), src_mask.to(const.device))[1:-1]\n",
    "\n",
    "    return embedding[:, 0, :].cpu().detach().numpy()\n",
    "    # return (sum(embedding) / len(embedding))[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     аванпост\n",
       "1       авиоас\n",
       "2      авиобос\n",
       "3    авиобранш\n",
       "4     авиоград\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv('/mnt/d/Projects/masters-thesis/data/words.csv', header=None, names=['word', 'transcription'])\n",
    "words['word'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(word_1, word_2):\n",
    "    m = len(word_1)\n",
    "    n = len(word_2)\n",
    "\n",
    "    dist = np.zeros((m + 1, n + 1))\n",
    "\n",
    "    dist[1:, 0] = list(accumulate(word_1, func=lambda x, y: x + np.linalg.norm(y), initial=0))[1:]\n",
    "    dist[0, 1:] = list(accumulate(word_2, func=lambda x, y: x + np.linalg.norm(y), initial=0))[1:]\n",
    "\n",
    "\n",
    "    for j in range(1, n+1):\n",
    "        for i in range(1, m+1):\n",
    "            deletion_cost = np.linalg.norm(word_2[j-1])  # Needs check\n",
    "            insertion_cost = np.linalg.norm(word_1[i-1])  # Needs check\n",
    "            substitution_cost = 0\n",
    "\n",
    "            distance = np.linalg.norm(word_1[i-1] - word_2[j-1])\n",
    "            if distance > 10:\n",
    "                substitution_cost =distance\n",
    "\n",
    "            dist[i, j] = min(dist[i - 1, j] + deletion_cost, \n",
    "                             dist[i, j - 1] + insertion_cost, \n",
    "                             dist[i - 1, j - 1] + substitution_cost)\n",
    "\n",
    "    # print(dist)\n",
    "    return dist[len(word_1), len(word_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(word_1, word_2):\n",
    "    word_1_embedding = get_embedding(transformer, word_1)\n",
    "    word_2_embedding = get_embedding(transformer, word_2)\n",
    "\n",
    "    return levenshtein(word_1_embedding, word_2_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_only = words.drop('transcription', axis=1).sample(10000)\n",
    "\n",
    "# words_only.merge(words_only, how='cross')\n",
    "cross = words_only.merge(words_only, how='cross', suffixes=['_1', '_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cross[\u001b[39m'\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cross\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: distance(x[\u001b[39m'\u001b[39;49m\u001b[39mword_1\u001b[39;49m\u001b[39m'\u001b[39;49m], x[\u001b[39m'\u001b[39;49m\u001b[39mword_2\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cross[\u001b[39m'\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cross\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: distance(x[\u001b[39m'\u001b[39;49m\u001b[39mword_1\u001b[39;49m\u001b[39m'\u001b[39;49m], x[\u001b[39m'\u001b[39;49m\u001b[39mword_2\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mdistance\u001b[0;34m(word_1, word_2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistance\u001b[39m(word_1, word_2):\n\u001b[1;32m      2\u001b[0m     word_1_embedding \u001b[39m=\u001b[39m get_embedding(transformer, word_1)\n\u001b[0;32m----> 3\u001b[0m     word_2_embedding \u001b[39m=\u001b[39m get_embedding(transformer, word_2)\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m levenshtein(word_1_embedding, word_2_embedding)\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(model, word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(model: Seq2SeqTransformer, word: \u001b[39mstr\u001b[39m):\n\u001b[1;32m      2\u001b[0m     word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39mlower()\n\u001b[0;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m      4\u001b[0m     src \u001b[39m=\u001b[39m text_transform_src(word)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(const\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m     num_tokens \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2307\u001b[0m, in \u001b[0;36mModule.eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval\u001b[39m(\u001b[39mself\u001b[39m: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m   2292\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sets the module in evaluation mode.\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m \n\u001b[1;32m   2294\u001b[0m \u001b[39m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2305\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m   2306\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.train at line 2288 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2288\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m-> 2288\u001b[0m     module\u001b[39m.\u001b[39;49mtrain(mode)\n\u001b[1;32m   2289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2286\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(mode, \u001b[39mbool\u001b[39m):\n\u001b[1;32m   2285\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtraining mode is expected to be boolean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39m=\u001b[39m mode\n\u001b[1;32m   2287\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m   2288\u001b[0m     module\u001b[39m.\u001b[39mtrain(mode)\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1626\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1623\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1624\u001b[0m                 d\u001b[39m.\u001b[39mdiscard(name)\n\u001b[0;32m-> 1626\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_parameters\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m   1627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Parameter):\n\u001b[1;32m   1628\u001b[0m     \u001b[39mif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross['distance'] = cross.apply(lambda x: distance(x['word_1'], x['word_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = cross[cross['distance'] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'мимоза': 425.3814010620117})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = defaultdict(float)\n",
    "\n",
    "for item in cross.iloc:\n",
    "    result[item['word_1']] += item['distance']\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
