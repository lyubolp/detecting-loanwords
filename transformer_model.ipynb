{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Iterable, List\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "# Local application imports\n",
    "import src.constants as const\n",
    "from src.dataset_utils import yield_tokens, sentence_to_tensor, load_files, build_vocab_transformation, tokenize_source, tokenize_target\n",
    "from src.training_utils import train_epoch, evaluate, sequential_transforms, tensor_transform\n",
    "from src.transcription_dataset import TranscriptionDataset\n",
    "from src.transformer_model import Seq2SeqTransformer, generate_square_subsequent_mask, create_mask\n",
    "from src.syllable_splitter import split_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Брой файлове: 47825, брой редове: 80615534'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the files\n",
    "files = load_files('/mnt/d/Projects/masters-thesis/data/transcriptions')\n",
    "\n",
    "filepaths_to_size = pd.read_csv('/mnt/d/Projects/masters-thesis/data/filepath_to_size.csv')\n",
    "lines_count = filepaths_to_size['size'].sum()\n",
    "\n",
    "f'Брой файлове: {len(files)}, брой редове: {lines_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & test split\n",
    "sentences_to_use = 50000\n",
    "train_split = int(const.TRAIN_TEST_SPLIT * sentences_to_use)\n",
    "validation_split = int((const.TRAIN_TEST_SPLIT + const.TRAIN_VALIDATION_SPLIT) * sentences_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranscriptionDataset(files, tokenization_src=tokenize_source, tokenization_tgt=tokenize_target,\n",
    "                                     start_index=0, end_index=train_split)\n",
    "validation_dataset = TranscriptionDataset(files, tokenization_src=tokenize_source, tokenization_tgt=tokenize_target,\n",
    "                                          start_index=train_split, end_index=validation_split)\n",
    "test_dataset = TranscriptionDataset(files, tokenization_src=tokenize_source, tokenization_tgt=tokenize_target,\n",
    "                                    start_index=validation_split, end_index=sentences_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [const.SRC_LANGUAGE, const.TGT_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object\n",
    "    const.vocab_transform[ln] = build_vocab_transformation(train_dataset, ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform_src = sequential_transforms(const.vocab_transform[const.SRC_LANGUAGE], #Numericalization\n",
    "                                                tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "vowels_transcription = ['a', 'ʌ', 'ɤ̞',  'ɐ', 'ɔ', 'o', 'u', 'ɛ', 'i']\n",
    "text_transform_tgt = sequential_transforms(const.vocab_transform[const.TGT_LANGUAGE], #Numericalization\n",
    "                                                tensor_transform) # Add BOS/EOS and create tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform_src(src_sample))\n",
    "        tgt_batch.append(text_transform_tgt(tgt_sample))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=const.PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=const.PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(deepcopy(train_dataset), batch_size=128, collate_fn=collate_fn)\n",
    "validation_dataloader = DataLoader(deepcopy(validation_dataset), batch_size=128, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(deepcopy(test_dataset), batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(const.vocab_transform[const.SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(const.vocab_transform[const.TGT_LANGUAGE])\n",
    "\n",
    "\n",
    "transformer = Seq2SeqTransformer(const.NUM_ENCODER_LAYERS, const.NUM_DECODER_LAYERS, const.EMB_SIZE,\n",
    "                                 const.NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, const.FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(const.device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=const.PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_dataloader, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, validation_dataloader, loss_fn)\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "torch.save(transformer.state_dict(), f'models/transformer-{today_date}-{sentences_to_use}-{NUM_EPOCHS}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load('models/transformer-2023-10-08-50000-25.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform_src = sequential_transforms(tokenize_source,\n",
    "    const.vocab_transform[const.SRC_LANGUAGE], #Numericalization\n",
    "                                                tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "vowels_transcription = ['a', 'ʌ', 'ɤ̞',  'ɐ', 'ɔ', 'o', 'u', 'ɛ', 'i']\n",
    "text_transform_tgt = sequential_transforms(const.vocab_transform[const.TGT_LANGUAGE], #Numericalization\n",
    "                                                tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(const.device)\n",
    "    src_mask = src_mask.to(const.device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(const.device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(const.device)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(const.device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == const.EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    src_sentence = src_sentence.lower()\n",
    "    model.eval()\n",
    "    src = text_transform_src(src_sentence).view(-1, 1)\n",
    "\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=const.BOS_IDX).flatten()\n",
    "    # print(list(tgt_tokens.cpu().numpy()))\n",
    "    return \" \".join(const.vocab_transform[const.TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zdrʌ vɛj , kʌk stɛ ? \n",
      " dob rɛ sɐm , blʌ go dʌr jɐ . \n",
      " xvɐr ljɐ \n",
      " bɐl gʌr ski ɛ zik \n",
      " ɛ zik bɐl gʌr ski ɛ zik \n",
      " ljɐ to to ɛ mo ɛ to ljo bi mo vrɛ mɛ nʌ go di nʌ tʌ . \n",
      " v pʌr kʌ rʌz lit tjɐ xʌ nɛ vɛ ro jɐt ni tsvɛt jɐ . \n",
      " mo zi kʌ tʌ os po ko jɐ vʌ do ʃʌ tʌ mi slɛd dɐ lɐg rʌ bo tɛn dɛn . \n",
      " vtʃɛ rʌ sɛ srɛʃ tnʌx sɐs stʌr pri jɐ tɛl , ko go to nɛ bjɐx viʒ dʌl go di ni . \n",
      " tʃɛ tɛ nɛ to nʌ kni gi rʌz ʃir jɐ vʌ xo ri zon ti tɛ i o bo gʌt jɐ vʌ rɛt ʃni kʌ . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"здравей, как сте?\"))\n",
    "print(translate(transformer, \"Добре съм, благодаря.\"))\n",
    "print(translate(transformer, \"Айрян\"))\n",
    "print(translate(transformer, \"Български език\"))\n",
    "print(translate(transformer, \"език Български език\"))\n",
    "print(translate(transformer, \"Лятото е моето любимо време на годината.\"))\n",
    "print(translate(transformer, \"В парка разцъфтяха невероятни цветя.\"))\n",
    "print(translate(transformer, \"Музиката успокоява душата ми след дълъг работен ден.\"))\n",
    "print(translate(transformer, \"Вчера се срещнах със стар приятел, когото не бях виждал години.\"))\n",
    "print(translate(transformer, \"Четенето на книги разширява хоризонтите и обогатява речника.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
