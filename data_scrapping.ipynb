{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Set, Optional\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изречения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file_sentences(filename) -> Set[str]:\n",
    "    lines = read_file(filename)\n",
    "\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line for line in lines if line != '']\n",
    "\n",
    "    tokens_per_line = [sent_tokenize(line) for line in lines]\n",
    "\n",
    "    print(tokens_per_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = sum([[os.path.join(dirpath, filename) for filename in filenames] for dirpath, _, filenames in os.walk('data/books')], [])\n",
    "\n",
    "f'Amount of books: {len(all_filenames)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_book(book_path: str) -> List[str]:\n",
    "    print(book_path)\n",
    "    with open(book_path, 'r') as file_descriptor:\n",
    "        lines = file_descriptor.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [line for line in lines if line != '']\n",
    "\n",
    "        sentences = sum([sent_tokenize(line) for line in lines], [])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текстовете съдържат форматиране, което е специфично за Читанка, и трябва да се премахне. Описано е [тук](https://forum.chitanka.info/topic511.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_symbols = ['E>', 'E$', 'D>', 'D$', '@', 'C>', 'C$', 'P>', 'P$', \n",
    "                      'S>', 'S$', '\\t', '|', '>', '#']\n",
    "\n",
    "def cleanup_formatting(sentence: str) -> str:\n",
    "    for symbol in formatting_symbols:\n",
    "        while symbol in sentence:\n",
    "            sentence = sentence.replace(symbol, '')\n",
    "    return sentence\n",
    "\n",
    "def tokenize_sentence(sentence: str, word_to_id: dict[str, int]) -> List[str]:\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    tokenized = word_tokenize(sentence)\n",
    "\n",
    "    for token in tokenized:\n",
    "        if token not in word_to_id:\n",
    "            word_to_id[token] = len(word_to_id)\n",
    "        \n",
    "    tokenized = [word_to_id[token] for token in tokenized]\n",
    "    return sentence, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "sample_books = get_sentences_from_book(all_filenames[0])\n",
    "sample_books = [cleanup_formatting(sentence) for sentence in sample_books]\n",
    "sample_books = [tokenize_sentence(sentence, word_to_id) for sentence in sample_books if sentence != '']\n",
    "\n",
    "sample_books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ударения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_emphasis(word: str):\n",
    "    \"\"\"\n",
    "    Input: а`виобра`нш\n",
    "    Output: [0, 6]\n",
    "    \"\"\"\n",
    "    emphasis_positions = [i for i, letter in enumerate(word) if letter == '`']\n",
    "\n",
    "    # i' = i - 1 - amount of previous ` symbols\n",
    "    corrected_emphasis_positions = tuple(position - 1 - i for i, position in enumerate(emphasis_positions))\n",
    "\n",
    "    return corrected_emphasis_positions\n",
    "\n",
    "parse_emphasis('а`виобра`нш')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('data/words.csv', header=None, names=['word', 'transcription'])\n",
    "\n",
    "\n",
    "words['emphasis_indexes'] = words['transcription'].apply(parse_emphasis)\n",
    "words.drop(columns=['transcription'], inplace=True)\n",
    "\n",
    "\n",
    "words.to_csv('data/emphasis.csv', index=False)\n",
    "words.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcription_generation import TranscriptionGeneration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генериране на двойки изречение - ударение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_from_book(book_path: str, transcription_generator: TranscriptionGeneration, \n",
    "                            word_to_id: dict[str, int], transcription_to_id: dict[str, int]):\n",
    "    senteces = get_sentences_from_book(book_path)\n",
    "    senteces = [cleanup_formatting(sentence) for sentence in senteces]\n",
    "    senteces = [tokenize_sentence(sentence, word_to_id) for sentence in senteces if sentence != '']\n",
    "\n",
    "    tokens = [token for _, token in senteces]   \n",
    "    senteces = [sentence for sentence, _ in senteces]\n",
    "    transcriptions = [transcription_generator.generate_transcription(sentence) for sentence in senteces]\n",
    "\n",
    "    tokenized_transcription = [tokenize_sentence(transcription, transcription_to_id)[1] for transcription in transcriptions]\n",
    "    df = pd.DataFrame({'sentence': senteces, 'tokens': tokens, \n",
    "                       'transcription': transcriptions, 'transcription_tokens': tokenized_transcription})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/transcriptions'):\n",
    "    os.makedirs('data/transcriptions')\n",
    "    word_to_id = {}\n",
    "    transcription_to_id = {}\n",
    "\n",
    "    transcription = TranscriptionGeneration()\n",
    "    for filename in tqdm(all_filenames):\n",
    "        df = generate_data_from_book(filename, transcription, word_to_id, transcription_to_id)\n",
    "        new_filename = filename.replace('books', 'transcriptions')\n",
    "        os.makedirs(os.path.dirname(new_filename), exist_ok=True)\n",
    "        df.to_csv(new_filename, index=False)\n",
    "    \n",
    "    word_to_id_df = pd.DataFrame({'word': list(word_to_id.keys()), 'id': list(word_to_id.values())})\n",
    "    word_to_id_df.to_csv('data/word_to_id.csv', index=False)\n",
    "\n",
    "    transcription_to_id_df = pd.DataFrame({'transcription': list(transcription_to_id.keys()),\n",
    "                                             'id': list(transcription_to_id.values())})\n",
    "    transcription_to_id_df.to_csv('data/transcription_to_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sch data/transcriptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(root, file)  for root, dirs, files in os.walk('data/transcriptions') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(files[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabilaries = pd.read_csv('data/word_to_id.csv')\n",
    "vocabilaries.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Amount of books: {len(all_filenames)}')\n",
    "print(f'Amount of different tokens: {vocabilaries.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amount_of_entries(filepath: str) -> int:\n",
    "    with open(filepath) as fp:\n",
    "        return sum(1 for _ in fp) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_to_size = {filename: get_amount_of_entries(filename) for filename in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_to_size_df = pd.DataFrame({'filepath': list(filepath_to_size.keys()),\n",
    "                                             'size': list(filepath_to_size.values())})\n",
    "filepath_to_size_df.to_csv('data/filepath_to_size.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcription_generation import TranscriptionGeneration\n",
    "\n",
    "transcription = TranscriptionGeneration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аванпост</td>\n",
       "      <td>ʌvʌnpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>авиоас</td>\n",
       "      <td>ʌvioʌs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>авиобос</td>\n",
       "      <td>ʌviobos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>авиобранш</td>\n",
       "      <td>ʌviobrʌnʃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авиоград</td>\n",
       "      <td>ʌviogrʌd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word transcription\n",
       "0   аванпост      ʌvʌnpost\n",
       "1     авиоас        ʌvioʌs\n",
       "2    авиобос       ʌviobos\n",
       "3  авиобранш     ʌviobrʌnʃ\n",
       "4   авиоград      ʌviogrʌd"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv('data/emphasis.csv').drop(columns=['emphasis_indexes'])\n",
    "\n",
    "words['transcription'] = words['word'].apply(transcription.generate_transcription)\n",
    "words.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word(word: str, word_to_id: dict[str, int]) -> List[str]:\n",
    "    for token in word:\n",
    "        if token not in word_to_id:\n",
    "            word_to_id[token] = len(word_to_id)\n",
    "        \n",
    "    vectorized = [word_to_id[token] for token in word]\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>transcription</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>transcription_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аванпост</td>\n",
       "      <td>ʌvʌnpost</td>\n",
       "      <td>[0, 1, 0, 2, 3, 4, 5, 6]</td>\n",
       "      <td>[0, 1, 0, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>авиоас</td>\n",
       "      <td>ʌvioʌs</td>\n",
       "      <td>[0, 1, 7, 4, 0, 5]</td>\n",
       "      <td>[0, 1, 7, 4, 0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>авиобос</td>\n",
       "      <td>ʌviobos</td>\n",
       "      <td>[0, 1, 7, 4, 8, 4, 5]</td>\n",
       "      <td>[0, 1, 7, 4, 8, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>авиобранш</td>\n",
       "      <td>ʌviobrʌnʃ</td>\n",
       "      <td>[0, 1, 7, 4, 8, 9, 0, 2, 10]</td>\n",
       "      <td>[0, 1, 7, 4, 8, 9, 0, 2, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авиоград</td>\n",
       "      <td>ʌviogrʌd</td>\n",
       "      <td>[0, 1, 7, 4, 11, 9, 0, 12]</td>\n",
       "      <td>[0, 1, 7, 4, 11, 9, 0, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word transcription                   word_vector  \\\n",
       "0   аванпост      ʌvʌnpost      [0, 1, 0, 2, 3, 4, 5, 6]   \n",
       "1     авиоас        ʌvioʌs            [0, 1, 7, 4, 0, 5]   \n",
       "2    авиобос       ʌviobos         [0, 1, 7, 4, 8, 4, 5]   \n",
       "3  авиобранш     ʌviobrʌnʃ  [0, 1, 7, 4, 8, 9, 0, 2, 10]   \n",
       "4   авиоград      ʌviogrʌd    [0, 1, 7, 4, 11, 9, 0, 12]   \n",
       "\n",
       "           transcription_vector  \n",
       "0      [0, 1, 0, 2, 3, 4, 5, 6]  \n",
       "1            [0, 1, 7, 4, 0, 5]  \n",
       "2         [0, 1, 7, 4, 8, 4, 5]  \n",
       "3  [0, 1, 7, 4, 8, 9, 0, 2, 10]  \n",
       "4    [0, 1, 7, 4, 11, 9, 0, 12]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "transcription_to_id = {}\n",
    "\n",
    "words['word_vector'] = words['word'].apply(vectorize_word, args=(word_to_id,))\n",
    "words['transcription_vector'] = words['transcription'].apply(vectorize_word, args=(transcription_to_id,))\n",
    "\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.to_csv('data/word-based/words.csv', index=False)\n",
    "\n",
    "word_to_id_df = pd.DataFrame({'word': list(word_to_id.keys()), 'id': list(word_to_id.values())})\n",
    "word_to_id_df.to_csv('data/word-based/word_to_id.csv', index=False)\n",
    "\n",
    "transcription_to_id_df = pd.DataFrame({'transcription': list(transcription_to_id.keys()), 'id': list(transcription_to_id.values())})\n",
    "transcription_to_id_df.to_csv('data/word-based/transcription_to_id.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9690224c8ba08809808eb1dea2a550f7a1d7415f0de077dd69f40462bd6d7bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
