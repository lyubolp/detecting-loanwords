{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import accumulate\n",
    "from typing import List, Set, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import transformer_model.src.constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изречения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file_sentences(filename) -> Set[str]:\n",
    "    lines = read_file(filename)\n",
    "\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line for line in lines if line != '']\n",
    "\n",
    "    tokens_per_line = [sent_tokenize(line) for line in lines]\n",
    "\n",
    "    print(tokens_per_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amount of books: 47825'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames = sum([[os.path.join(dirpath, filename) for filename in filenames] for dirpath, _, filenames in os.walk('/mnt/d/Projects/masters-thesis/data/transcriptions')], [])\n",
    "\n",
    "f'Amount of books: {len(all_filenames)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_book(book_path: str) -> List[str]:\n",
    "    print(book_path)\n",
    "    with open(book_path, 'r') as file_descriptor:\n",
    "        lines = file_descriptor.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [line for line in lines if line != '']\n",
    "\n",
    "        sentences = sum([sent_tokenize(line) for line in lines], [])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текстовете съдържат форматиране, което е специфично за Читанка, и трябва да се премахне. Описано е [тук](https://forum.chitanka.info/topic511.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_symbols = ['E>', 'E$', 'D>', 'D$', '@', 'C>', 'C$', 'P>', 'P$', \n",
    "                      'S>', 'S$', '\\t', '|', '>', '#']\n",
    "\n",
    "def cleanup_formatting(sentence: str) -> str:\n",
    "    for symbol in formatting_symbols:\n",
    "        while symbol in sentence:\n",
    "            sentence = sentence.replace(symbol, '')\n",
    "    return sentence\n",
    "\n",
    "def tokenize_sentence(sentence: str, word_to_id: dict[str, int]) -> List[str]:\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    tokenized = word_tokenize(sentence)\n",
    "\n",
    "    for token in tokenized:\n",
    "        if token not in word_to_id:\n",
    "            word_to_id[token] = len(word_to_id)\n",
    "        \n",
    "    tokenized = [word_to_id[token] for token in tokenized]\n",
    "    return sentence, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "sample_books = get_sentences_from_book(all_filenames[0])\n",
    "sample_books = [cleanup_formatting(sentence) for sentence in sample_books]\n",
    "sample_books = [tokenize_sentence(sentence, word_to_id) for sentence in sample_books if sentence != '']\n",
    "\n",
    "sample_books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ударения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_emphasis(word: str):\n",
    "    \"\"\"\n",
    "    Input: а`виобра`нш\n",
    "    Output: [0, 6]\n",
    "    \"\"\"\n",
    "    emphasis_positions = [i for i, letter in enumerate(word) if letter == '`']\n",
    "\n",
    "    # i' = i - 1 - amount of previous ` symbols\n",
    "    corrected_emphasis_positions = tuple(position - 1 - i for i, position in enumerate(emphasis_positions))\n",
    "\n",
    "    return corrected_emphasis_positions\n",
    "\n",
    "parse_emphasis('а`виобра`нш')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('data/words.csv', header=None, names=['word', 'transcription'])\n",
    "\n",
    "\n",
    "words['emphasis_indexes'] = words['transcription'].apply(parse_emphasis)\n",
    "words.drop(columns=['transcription'], inplace=True)\n",
    "\n",
    "\n",
    "words.to_csv('data/emphasis.csv', index=False)\n",
    "words.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcription_generation import TranscriptionGeneration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генериране на двойки изречение - ударение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_from_book(book_path: str, transcription_generator: TranscriptionGeneration, \n",
    "                            word_to_id: dict[str, int], transcription_to_id: dict[str, int]):\n",
    "    senteces = get_sentences_from_book(book_path)\n",
    "    senteces = [cleanup_formatting(sentence) for sentence in senteces]\n",
    "    senteces = [tokenize_sentence(sentence, word_to_id) for sentence in senteces if sentence != '']\n",
    "\n",
    "    tokens = [token for _, token in senteces]   \n",
    "    senteces = [sentence for sentence, _ in senteces]\n",
    "    transcriptions = [transcription_generator.generate_transcription(sentence) for sentence in senteces]\n",
    "\n",
    "    tokenized_transcription = [tokenize_sentence(transcription, transcription_to_id)[1] for transcription in transcriptions]\n",
    "    df = pd.DataFrame({'sentence': senteces, 'tokens': tokens, \n",
    "                       'transcription': transcriptions, 'transcription_tokens': tokenized_transcription})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/transcriptions'):\n",
    "    os.makedirs('data/transcriptions')\n",
    "    word_to_id = {}\n",
    "    transcription_to_id = {}\n",
    "\n",
    "    transcription = TranscriptionGeneration()\n",
    "    for filename in tqdm(all_filenames):\n",
    "        df = generate_data_from_book(filename, transcription, word_to_id, transcription_to_id)\n",
    "        new_filename = filename.replace('books', 'transcriptions')\n",
    "        os.makedirs(os.path.dirname(new_filename), exist_ok=True)\n",
    "        df.to_csv(new_filename, index=False)\n",
    "    \n",
    "    word_to_id_df = pd.DataFrame({'word': list(word_to_id.keys()), 'id': list(word_to_id.values())})\n",
    "    word_to_id_df.to_csv('data/word_to_id.csv', index=False)\n",
    "\n",
    "    transcription_to_id_df = pd.DataFrame({'transcription': list(transcription_to_id.keys()),\n",
    "                                             'id': list(transcription_to_id.values())})\n",
    "    transcription_to_id_df.to_csv('data/transcription_to_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sch data/transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(root, file)  for root, dirs, files in os.walk('/mnt/d/Projects/masters-thesis/data/transcriptions') for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amount_of_entries(filepath: str) -> int:\n",
    "    with open(filepath) as fp:\n",
    "        return sum(1 for _ in fp) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filepath_to_size \u001b[39m=\u001b[39m {filename: get_amount_of_entries(filename) \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m files}\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filepath_to_size \u001b[39m=\u001b[39m {filename: get_amount_of_entries(filename) \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m files}\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mget_amount_of_entries\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_amount_of_entries\u001b[39m(filepath: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filepath) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m      3\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m fp) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/detecting-loanwords/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[1;32m    304\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m    byte sequences.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    310\u001b[0m         IncrementalDecoder\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors)\n\u001b[1;32m    311\u001b[0m         \u001b[39m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath_to_size = {filename: get_amount_of_entries(filename) for filename in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepath_to_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>size</th>\n",
       "      <th>end_index</th>\n",
       "      <th>start_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/d/Projects/masters-thesis/data/transcript...</td>\n",
       "      <td>10731</td>\n",
       "      <td>10731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/Projects/masters-thesis/data/transcript...</td>\n",
       "      <td>136</td>\n",
       "      <td>10867</td>\n",
       "      <td>10731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/d/Projects/masters-thesis/data/transcript...</td>\n",
       "      <td>13136</td>\n",
       "      <td>24003</td>\n",
       "      <td>10867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/d/Projects/masters-thesis/data/transcript...</td>\n",
       "      <td>12301</td>\n",
       "      <td>36304</td>\n",
       "      <td>24003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/Projects/masters-thesis/data/transcript...</td>\n",
       "      <td>12298</td>\n",
       "      <td>48602</td>\n",
       "      <td>36304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath   size  end_index  \\\n",
       "0  /mnt/d/Projects/masters-thesis/data/transcript...  10731      10731   \n",
       "1  /mnt/d/Projects/masters-thesis/data/transcript...    136      10867   \n",
       "2  /mnt/d/Projects/masters-thesis/data/transcript...  13136      24003   \n",
       "3  /mnt/d/Projects/masters-thesis/data/transcript...  12301      36304   \n",
       "4  /mnt/d/Projects/masters-thesis/data/transcript...  12298      48602   \n",
       "\n",
       "   start_index  \n",
       "0            0  \n",
       "1        10731  \n",
       "2        10867  \n",
       "3        24003  \n",
       "4        36304  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_to_size_df = pd.DataFrame({'filepath': list(filepath_to_size.keys()),\n",
    "                                             'size': list(filepath_to_size.values())})\n",
    "\n",
    "filepath_to_size_df = filepath_to_size_df.sort_values(by=['filepath'])\n",
    "filepath_to_size_df['end_index'] = list(accumulate(filepath_to_size_df['size']))\n",
    "filepath_to_size_df['start_index'] = [0] + list(filepath_to_size_df['end_index'].iloc()[:-1])\n",
    "\n",
    "filepath_to_size_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64492427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_count = filepath_to_size_df['size'].sum()\n",
    "split = int(const.TRAIN_TEST_SPLIT * lines_count)\n",
    "\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(const.TRAIN_TEST_SPLIT * lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_to_size_df.to_csv('/mnt/d/Projects/masters-thesis/data/filepath_to_size.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>тук е пълно с гении, докторе.</td>\n",
       "      <td>[283, 65, 530, 187, 129444, 8, 44327, 58]</td>\n",
       "      <td>tok ɛ pɐlno s gɛnii , doktorɛ .</td>\n",
       "      <td>[283, 65, 529, 187, 128247, 8, 44188, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>тук всеки е написал нещо изумително, или изобр...</td>\n",
       "      <td>[283, 382, 65, 3264, 119, 56219, 8, 396, 68696...</td>\n",
       "      <td>tok vsɛki ɛ nʌpisʌl nɛʃto izomitɛlno , ili izo...</td>\n",
       "      <td>[283, 382, 65, 3258, 119, 56029, 8, 396, 68443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>тук ухае на велики идеи и велики творби, пък а...</td>\n",
       "      <td>[283, 67329, 3, 15763, 4280, 5, 15763, 7793, 8...</td>\n",
       "      <td>tok oxʌɛ nʌ vɛliki idɛi i vɛliki tvorbi , pɐk ...</td>\n",
       "      <td>[283, 67085, 3, 15716, 4271, 5, 15716, 7776, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>тънък и дълъг дървен цилиндър.</td>\n",
       "      <td>[3446, 5, 1072, 16521, 12915, 58]</td>\n",
       "      <td>tɐnɐk i dɐlɐg dɐrvɛn tsilindɐr .</td>\n",
       "      <td>[3440, 5, 1069, 16473, 12879, 58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>долу съм вкопан в земята, затова дойдох при те...</td>\n",
       "      <td>[2072, 28, 51855, 30, 573, 8, 59, 8749, 138, 1...</td>\n",
       "      <td>dolo sɐm vkopʌn v zɛmjɐtʌ , zʌtovʌ dojdox pri ...</td>\n",
       "      <td>[1558, 28, 51680, 30, 572, 8, 59, 8730, 138, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                      тук е пълно с гении, докторе.   \n",
       "1  тук всеки е написал нещо изумително, или изобр...   \n",
       "2  тук ухае на велики идеи и велики творби, пък а...   \n",
       "3                     тънък и дълъг дървен цилиндър.   \n",
       "4  долу съм вкопан в земята, затова дойдох при те...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0          [283, 65, 530, 187, 129444, 8, 44327, 58]   \n",
       "1  [283, 382, 65, 3264, 119, 56219, 8, 396, 68696...   \n",
       "2  [283, 67329, 3, 15763, 4280, 5, 15763, 7793, 8...   \n",
       "3                  [3446, 5, 1072, 16521, 12915, 58]   \n",
       "4  [2072, 28, 51855, 30, 573, 8, 59, 8749, 138, 1...   \n",
       "\n",
       "                                       transcription  \\\n",
       "0                    tok ɛ pɐlno s gɛnii , doktorɛ .   \n",
       "1  tok vsɛki ɛ nʌpisʌl nɛʃto izomitɛlno , ili izo...   \n",
       "2  tok oxʌɛ nʌ vɛliki idɛi i vɛliki tvorbi , pɐk ...   \n",
       "3                   tɐnɐk i dɐlɐg dɐrvɛn tsilindɐr .   \n",
       "4  dolo sɐm vkopʌn v zɛmjɐtʌ , zʌtovʌ dojdox pri ...   \n",
       "\n",
       "                                transcription_tokens  \n",
       "0          [283, 65, 529, 187, 128247, 8, 44188, 58]  \n",
       "1  [283, 382, 65, 3258, 119, 56029, 8, 396, 68443...  \n",
       "2  [283, 67085, 3, 15716, 4271, 5, 15716, 7776, 8...  \n",
       "3                  [3440, 5, 1069, 16473, 12879, 58]  \n",
       "4  [1558, 28, 51680, 30, 572, 8, 59, 8730, 138, 1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(files[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2648747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabilaries = pd.read_csv('/mnt/d/Projects/masters-thesis/data/word_to_id.csv')\n",
    "vocabilaries.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of books: 47825\n",
      "Amount of different tokens: 2648747\n",
      "80615534\n"
     ]
    }
   ],
   "source": [
    "print(f'Amount of books: {len(all_filenames)}')\n",
    "print(f'Amount of different tokens: {vocabilaries.shape[0]}')\n",
    "\n",
    "filepath_to_size_path = '/mnt/d/Projects/masters-thesis/data/filepath_to_size.csv'\n",
    "if os.path.exists(filepath_to_size_path):\n",
    "    filepath_to_size = pd.read_csv(filepath_to_size_path)\n",
    "    print(filepath_to_size['size'].sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcription_generation import TranscriptionGeneration\n",
    "\n",
    "transcription = TranscriptionGeneration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аванпост</td>\n",
       "      <td>ʌvʌnpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>авиоас</td>\n",
       "      <td>ʌvioʌs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>авиобос</td>\n",
       "      <td>ʌviobos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>авиобранш</td>\n",
       "      <td>ʌviobrʌnʃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авиоград</td>\n",
       "      <td>ʌviogrʌd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word transcription\n",
       "0   аванпост      ʌvʌnpost\n",
       "1     авиоас        ʌvioʌs\n",
       "2    авиобос       ʌviobos\n",
       "3  авиобранш     ʌviobrʌnʃ\n",
       "4   авиоград      ʌviogrʌd"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv('data/emphasis.csv').drop(columns=['emphasis_indexes'])\n",
    "\n",
    "words['transcription'] = words['word'].apply(transcription.generate_transcription)\n",
    "words.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word(word: str, word_to_id: dict[str, int]) -> List[str]:\n",
    "    for token in word:\n",
    "        if token not in word_to_id:\n",
    "            word_to_id[token] = len(word_to_id)\n",
    "        \n",
    "    vectorized = [word_to_id[token] for token in word]\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>transcription</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>transcription_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>аванпост</td>\n",
       "      <td>ʌvʌnpost</td>\n",
       "      <td>[0, 1, 0, 2, 3, 4, 5, 6]</td>\n",
       "      <td>[0, 1, 0, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>авиоас</td>\n",
       "      <td>ʌvioʌs</td>\n",
       "      <td>[0, 1, 7, 4, 0, 5]</td>\n",
       "      <td>[0, 1, 7, 4, 0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>авиобос</td>\n",
       "      <td>ʌviobos</td>\n",
       "      <td>[0, 1, 7, 4, 8, 4, 5]</td>\n",
       "      <td>[0, 1, 7, 4, 8, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>авиобранш</td>\n",
       "      <td>ʌviobrʌnʃ</td>\n",
       "      <td>[0, 1, 7, 4, 8, 9, 0, 2, 10]</td>\n",
       "      <td>[0, 1, 7, 4, 8, 9, 0, 2, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авиоград</td>\n",
       "      <td>ʌviogrʌd</td>\n",
       "      <td>[0, 1, 7, 4, 11, 9, 0, 12]</td>\n",
       "      <td>[0, 1, 7, 4, 11, 9, 0, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word transcription                   word_vector  \\\n",
       "0   аванпост      ʌvʌnpost      [0, 1, 0, 2, 3, 4, 5, 6]   \n",
       "1     авиоас        ʌvioʌs            [0, 1, 7, 4, 0, 5]   \n",
       "2    авиобос       ʌviobos         [0, 1, 7, 4, 8, 4, 5]   \n",
       "3  авиобранш     ʌviobrʌnʃ  [0, 1, 7, 4, 8, 9, 0, 2, 10]   \n",
       "4   авиоград      ʌviogrʌd    [0, 1, 7, 4, 11, 9, 0, 12]   \n",
       "\n",
       "           transcription_vector  \n",
       "0      [0, 1, 0, 2, 3, 4, 5, 6]  \n",
       "1            [0, 1, 7, 4, 0, 5]  \n",
       "2         [0, 1, 7, 4, 8, 4, 5]  \n",
       "3  [0, 1, 7, 4, 8, 9, 0, 2, 10]  \n",
       "4    [0, 1, 7, 4, 11, 9, 0, 12]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id = {}\n",
    "transcription_to_id = {}\n",
    "\n",
    "words['word_vector'] = words['word'].apply(vectorize_word, args=(word_to_id,))\n",
    "words['transcription_vector'] = words['transcription'].apply(vectorize_word, args=(transcription_to_id,))\n",
    "\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.to_csv('data/word-based/words.csv', index=False)\n",
    "\n",
    "word_to_id_df = pd.DataFrame({'word': list(word_to_id.keys()), 'id': list(word_to_id.values())})\n",
    "word_to_id_df.to_csv('data/word-based/word_to_id.csv', index=False)\n",
    "\n",
    "transcription_to_id_df = pd.DataFrame({'transcription': list(transcription_to_id.keys()), 'id': list(transcription_to_id.values())})\n",
    "transcription_to_id_df.to_csv('data/word-based/transcription_to_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.dataset_utils import load_files\n",
    "\n",
    "files = load_files('/mnt/d/Projects/masters-thesis/data/transcriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9690224c8ba08809808eb1dea2a550f7a1d7415f0de077dd69f40462bd6d7bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
