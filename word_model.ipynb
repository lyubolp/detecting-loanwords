{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "SOS_TOKEN = 69\n",
    "EOS_TOKEN = 70\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self) -> torch.Tensor:\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(content, target_size = MAX_LENGTH) -> torch.Tensor:\n",
    "    # Add padding to the end of the sentence, so that the length is equal to target_size\n",
    "    tensor = torch.tensor(content, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "    if tensor.size()[0] < target_size:\n",
    "        padding = torch.zeros(target_size - tensor.size()[0], 1, dtype=torch.int32, device=device)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionDataset(Dataset):\n",
    "    def __init__(self, path_to_words: str = 'data/word-based/words.csv'):\n",
    "        self.__words = pd.read_csv(path_to_words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.__words.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__len__():\n",
    "            raise IndexError\n",
    "        \n",
    "        row = self.__words.iloc[idx]\n",
    "\n",
    "        input_tensor = sentence_to_tensor(json.loads(row[2]))\n",
    "        target_tensor = sentence_to_tensor(json.loads(row[3]))\n",
    "        return input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN,\n",
    "                 encoder_optimizer: optim.Optimizer, decoder_optimizer: optim.Optimizer,\n",
    "                 max_length: int = MAX_LENGTH):\n",
    "        self.__encoder = encoder.to(device)\n",
    "        self.__decoder = decoder.to(device)\n",
    "        self.__encoder_optimizer = encoder_optimizer\n",
    "        self.__decoder_optimizer = decoder_optimizer\n",
    "        self.__max_length = max_length\n",
    "        self.__loss = 0\n",
    "\n",
    "    def __init_train(self):\n",
    "        encoder_hidden = self.__encoder.init_hidden()\n",
    "\n",
    "        self.__encoder_optimizer.zero_grad()\n",
    "        self.__decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs = torch.zeros(self.__max_length, self.__encoder.hidden_size, device=device)\n",
    "\n",
    "        self.__loss = 0\n",
    "\n",
    "        return encoder_hidden, encoder_outputs\n",
    "    \n",
    "    def __encoder_train(self, encoder_outputs, input_tensor, encoder_hidden):\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.__encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        return encoder_outputs\n",
    "\n",
    "    def __optimizers_step(self):\n",
    "        self.__encoder_optimizer.step()\n",
    "        self.__decoder_optimizer.step()\n",
    "\n",
    "    def __decoder_train(self, decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion):\n",
    "        target_length = target_tensor.size(0)\n",
    "        use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.__decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target_tensor[di]\n",
    "            else:\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            self.__loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            if use_teacher_forcing and decoder_input.item() == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "    def train(self, input_tensor: Tensor, target_tensor: Tensor, \n",
    "              criterion: nn.Module, max_length: int = MAX_LENGTH) -> tuple[float, float]:        \n",
    "\n",
    "        # Encoder training\n",
    "        encoder_hidden, encoder_outputs = self.__init_train()\n",
    "        encoder_outputs = self.__encoder_train(encoder_outputs, input_tensor, encoder_hidden)\n",
    "\n",
    "        # Decoder training\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        self.__decoder_train(decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion)\n",
    "        \n",
    "        # Optimizers step\n",
    "        self.__loss.backward()\n",
    "        self.__optimizers_step()\n",
    "\n",
    "        return self.__loss.item() / target_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(encoder: EncoderRNN, decoder: DecoderRNN, dataset: TranscriptionDataset,\n",
    "               epochs: int, print_every: int = 100):\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    trainer = Trainer(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # for iteration, (input_tensor, target_tensor) in tqdm.tqdm(enumerate(dataset)):\n",
    "        for iteration, (input_tensor, target_tensor) in enumerate(dataset):\n",
    "            print(f'Input tensor shape: {input_tensor.shape}')\n",
    "            print(f'Target tensor shape: {target_tensor.shape}')\n",
    "            loss = trainer.train(input_tensor, target_tensor, criterion)\n",
    "            total_loss += loss\n",
    "\n",
    "            if iteration % print_every == 0:\n",
    "                print(f'Epoch: {epoch} Iteration: {iteration} loss: {loss}')\n",
    "                print(f'Average loss: {total_loss / iteration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 50\n",
    "output_size = 50\n",
    "\n",
    "encoder = EncoderRNN(MAX_LENGTH, HIDDEN_SIZE).to(device)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE, MAX_LENGTH).to(device)\n",
    "\n",
    "dataset = TranscriptionDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(encoder, decoder, dataset, 1, print_every=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9690224c8ba08809808eb1dea2a550f7a1d7415f0de077dd69f40462bd6d7bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
