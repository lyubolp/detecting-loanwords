{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MAX_LENGTH = 1000\n",
    "SOS_TOKEN = 2648747\n",
    "EOS_TOKEN = 2648748\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self) -> torch.Tensor:\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(content, target_size = MAX_LENGTH) -> torch.Tensor:\n",
    "    # Add padding to the end of the sentence, so that the length is equal to target_size\n",
    "    tensor = torch.tensor(content, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "    if tensor.size()[0] < target_size:\n",
    "        padding = torch.zeros(target_size - tensor.size()[0], 1, dtype=torch.int32, device=device)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.__files = files\n",
    "        self.__current_file = 0\n",
    "        self.__current_row = 0\n",
    "        self.__filepath_to_size = pd.read_csv('data/filepath_to_size.csv')\n",
    "        self.__max_rows = self.__get_total_amount_of_entries()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__max_rows:\n",
    "            raise IndexError\n",
    "\n",
    "        current_file_path = self.__files[self.__current_file]\n",
    "        print(current_file_path)\n",
    "        if idx == self.__get_amount_of_entries(current_file_path):\n",
    "            self.__current_file += 1\n",
    "            self.__current_row = 0\n",
    "\n",
    "            current_file_path = self.__files[self.__current_file]\n",
    "        \n",
    "        df = pd.read_csv(current_file_path)\n",
    "\n",
    "        row = df.iloc[self.__current_row]\n",
    "\n",
    "        input_tensor = sentence_to_tensor(json.loads(row[1]))\n",
    "        target_tensor = sentence_to_tensor(json.loads(row[3]))\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_amount_of_entries(filepath: str) -> int:\n",
    "        with open(filepath) as fp:\n",
    "            return sum(1 for _ in fp) - 1\n",
    "\n",
    "    def __get_total_amount_of_entries(self) -> int:\n",
    "        return self.__filepath_to_size['size'].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN,\n",
    "                 encoder_optimizer: optim.Optimizer, decoder_optimizer: optim.Optimizer,\n",
    "                 max_length: int = MAX_LENGTH):\n",
    "        self.__encoder = encoder.to(device)\n",
    "        self.__decoder = decoder.to(device)\n",
    "        self.__encoder_optimizer = encoder_optimizer\n",
    "        self.__decoder_optimizer = decoder_optimizer\n",
    "        self.__max_length = max_length\n",
    "        self.__loss = 0\n",
    "\n",
    "    def __init_train(self):\n",
    "        encoder_hidden = self.__encoder.init_hidden()\n",
    "\n",
    "        self.__encoder_optimizer.zero_grad()\n",
    "        self.__decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs = torch.zeros(self.__max_length, self.__encoder.hidden_size, device=device)\n",
    "\n",
    "        self.__loss = 0\n",
    "\n",
    "        return encoder_hidden, encoder_outputs\n",
    "    \n",
    "    def __encoder_train(self, encoder_outputs, input_tensor, encoder_hidden):\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.__encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        return encoder_outputs\n",
    "\n",
    "    def __optimizers_step(self):\n",
    "        self.__encoder_optimizer.step()\n",
    "        self.__decoder_optimizer.step()\n",
    "\n",
    "    def __decoder_train(self, decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion):\n",
    "        target_length = target_tensor.size(0)\n",
    "        use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.__decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target_tensor[di]\n",
    "            else:\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            self.__loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            if use_teacher_forcing and decoder_input.item() == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "    def train(self, input_tensor: Tensor, target_tensor: Tensor, \n",
    "              criterion: nn.Module, max_length: int = MAX_LENGTH) -> tuple[float, float]:        \n",
    "\n",
    "        # Encoder training\n",
    "        encoder_hidden, encoder_outputs = self.__init_train()\n",
    "        encoder_outputs = self.__encoder_train(encoder_outputs, input_tensor, encoder_hidden)\n",
    "\n",
    "\n",
    "        print(f'Encoder hidden size: {encoder_hidden.size()}')\n",
    "        print(f'Encoder outputs size: {encoder_outputs.size()}')\n",
    "        # Decoder training\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        self.__decoder_train(decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion)\n",
    "        \n",
    "        # Optimizers step\n",
    "        self.__loss.backward()\n",
    "        self.__optimizers_step()\n",
    "\n",
    "        return self.__loss.item() / target_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(encoder: EncoderRNN, decoder: DecoderRNN, dataset: TranscriptionDataset,\n",
    "               epochs: int, print_every: int = 100):\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    trainer = Trainer(encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # for iteration, (input_tensor, target_tensor) in tqdm.tqdm(enumerate(dataset)):\n",
    "        for iteration, (input_tensor, target_tensor) in enumerate(dataset):\n",
    "            print(f'Input tensor shape: {input_tensor.shape}')\n",
    "            print(f'Target tensor shape: {target_tensor.shape}')\n",
    "            loss = trainer.train(input_tensor, target_tensor, criterion)\n",
    "            total_loss += loss\n",
    "\n",
    "            if iteration % print_every == 0:\n",
    "                print(f'Epoch: {epoch} Iteration: {iteration} loss: {loss}')\n",
    "                print(f'Average loss: {total_loss / iteration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2648749\n",
    "output_size = 2557622\n",
    "\n",
    "encoder = EncoderRNN(MAX_LENGTH, HIDDEN_SIZE).to(device)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE, MAX_LENGTH).to(device)\n",
    "\n",
    "files = [os.path.join(root, file)  for root, dirs, files in os.walk('data/transcriptions') for file in files]\n",
    "\n",
    "dataset = TranscriptionDataset(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/transcriptions/0d/3425\n",
      "Input tensor shape: torch.Size([1000, 1])\n",
      "Target tensor shape: torch.Size([1000, 1])\n",
      "Encoder hidden size: torch.Size([1, 1, 256])\n",
      "Encoder outputs size: torch.Size([1000, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loop(encoder, decoder, dataset, \u001b[39m1\u001b[39;49m, print_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(encoder, decoder, dataset, epochs, print_every)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput tensor shape: \u001b[39m\u001b[39m{\u001b[39;00minput_tensor\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget tensor shape: \u001b[39m\u001b[39m{\u001b[39;00mtarget_tensor\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(input_tensor, target_tensor, criterion)\n\u001b[1;32m     18\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m iteration \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, input_tensor, target_tensor, criterion, max_length)\u001b[0m\n\u001b[1;32m     67\u001b[0m decoder_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[SOS_TOKEN]], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     68\u001b[0m decoder_hidden \u001b[39m=\u001b[39m encoder_hidden\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__decoder_train(decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion)\n\u001b[1;32m     72\u001b[0m \u001b[39m# Optimizers step\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__loss\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mTrainer.__decoder_train\u001b[0;34m(self, decoder_input, decoder_hidden, encoder_outputs, target_tensor, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m use_teacher_forcing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m TEACHER_FORCING_RATIO \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m di \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(target_length):\n\u001b[0;32m---> 42\u001b[0m     decoder_output, decoder_hidden, decoder_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__decoder(\n\u001b[1;32m     43\u001b[0m             decoder_input, decoder_hidden, encoder_outputs)\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m use_teacher_forcing:\n\u001b[1;32m     46\u001b[0m         decoder_input \u001b[39m=\u001b[39m target_tensor[di]\n",
      "File \u001b[0;32m~/Projects/masters-thesis/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden, encoder_outputs):\n\u001b[0;32m---> 34\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(\u001b[39minput\u001b[39;49m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedded)\n\u001b[1;32m     37\u001b[0m     attn_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(torch\u001b[39m.\u001b[39mcat((embedded[\u001b[39m0\u001b[39m], hidden[\u001b[39m0\u001b[39m]), \u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/masters-thesis/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/masters-thesis/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/Projects/masters-thesis/detecting-loanwords/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train_loop(encoder, decoder, dataset, 1, print_every=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9690224c8ba08809808eb1dea2a550f7a1d7415f0de077dd69f40462bd6d7bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
